{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import codecs\n",
    "\n",
    "import sys\n",
    "stdout = sys.stdout\n",
    "reload(sys)\n",
    "sys.setdefaultencoding('utf-8')\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import multiprocessing\n",
    "from konlpy.tag import Mecab\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chosun_fullfinished\n",
      "donga_fullfinished\n",
      "hani_fullfinished\n",
      "joongang_fullfinished\n",
      "kh_fullfinished\n"
     ]
    }
   ],
   "source": [
    "text = []\n",
    "mecab = Mecab()\n",
    "\n",
    "for news_name in ['chosun_full', 'donga_full', 'hani_full', 'joongang_full', 'kh_full'] :\n",
    "    with open('./Data/news/'+ news_name + '.txt', 'r') as f:\n",
    "        lines = f.read()\n",
    "        sentences = lines.split('.')\n",
    "        \n",
    "        for i in range(len(sentences)):\n",
    "            dic = mecab.morphs(sentences[i])\n",
    "            dicSize = len(dic)\n",
    "                    \n",
    "            for idx in range(dicSize-1):\n",
    "                if(idx==dicSize):\n",
    "                        break\n",
    "                \n",
    "                else:\n",
    "                    if(dic[idx]=='새' and dic[(idx+1)]=='정치') :\n",
    "                        dic.remove('새')\n",
    "                        dic.remove('정치')\n",
    "                        dic.insert(idx,\"새정치\")\n",
    "                        dicSize = dicSize-1\n",
    "            \n",
    "                text.append(dic)\n",
    "    print(news_name + \"finished\")\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time() \n",
    "\n",
    "\n",
    "modelCbow= Word2Vec(text, min_count=15, size = 100, sg=0, iter= 5) #sg=0 : cbow\n",
    "modelSkip = Word2Vec(text, min_count=15, size = 100, sg=1, iter= 5) #sg=1 : skip gram\n",
    "print(\"word2vec success\")\n",
    "\n",
    "vectorsCbow = modelCbow.wv\n",
    "vectorsSkip = modelSkip.wv\n",
    "print(\"model.wv assign success\")\n",
    "\n",
    "vectorsCbow.save('modelCbow.bin')\n",
    "vectorsSkip.save('modelSkip.bin')\n",
    "print(\"model saved success\")\n",
    "\n",
    "\n",
    "print(\"Elapsed time: %s seconds\" %(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**loading models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load('./original/modelCbow.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=\"\\xec\\xb5\\x9c\\xec\\x88\\x9c\\xec\\x8b\\xa4\".encode(\"utf-8\")\n",
    "print a\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word similarity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_list = [\"박근혜\",\"오바마\",\"김정은\",\"아베\",\"청와대\",\"백악관\",\"새누리당\",\"최순실\"]\n",
    "\n",
    "for idx in range(len(name_list)):\n",
    "    name_list[idx] = name_list[idx].encode(\"utf-8\")\n",
    "\n",
    "similarity_list = []\n",
    "for name in name_list:\n",
    "    name = name.encode(\"utf-8\")\n",
    "    w = word_vectors.most_similar(name, topn=1)\n",
    "    w1 = w[0][0]\n",
    "    w2 = w[0][1]\n",
    "    print \"[%s]'s most similar: [%s]\" %(name, w1),\n",
    "    print(w2)\n",
    "    similarity_list.append(\"[%s]'s most similar: [%s]\" % (name, (w)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word analogy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_analogy(w1, w2, w3):\n",
    "    return word_vectors.most_similar(positive=[w1, w2], negative=[w3], topn=1)\n",
    "\n",
    "words_list = [[\"서울\",\"일본\",\"한국\"],[\"서울\",\"미국\",\"한국\"],[\"서울\",\"북한\",\"한국\"],\n",
    "                [\"박근혜\",\"일본\",\"한국\"],[\"박근혜\",\"미국\",\"한국\"],[\"박근혜\",\"북한\",\"한국\"],\n",
    "                [\"새누리당\",\"미국\",\"한국\"],[\"청와대\",\"미국\",\"한국\"],[\"민주주의\",\"북한\",\"한국\"],\n",
    "                [\"김무성\",\"새정치\",\"새누리당\"],[\"보수\",\"새정치\",\"새누리당\"],[\"새정치\",\"미국\",\"한국\"]]\n",
    "\n",
    "for element in words_list:\n",
    "    for idx in range(len(element)):\n",
    "        element[idx] = element[idx].encode(\"utf-8\")\n",
    "\n",
    "d = {\"%s+%s-%s\"%(words[0],words[1],words[2]):word_analogy(words[0],words[1],words[2])[0][0] for words in words_list}\n",
    "\n",
    "for key in d:\n",
    "    print \"[%s] corresponds to [%s]\" % (key, d[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
